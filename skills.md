# Context Engineering Generator: Event Handler Service

**Goal:** To reverse-engineer an existing, working Event Handler Service into a reusable "Skills Folder" that allows an AI (or engineer) to rapidly replicate the architecture for future projects.

---

## 1. The Master Prompt
*Copy and paste the text below into your AI Agent (Cursor, GitHub Copilot Workspace, or ChatGPT with codebase access).*

***

**Role:** Senior Solutions Architect & LLM Context Engineer
**Objective:** Analyze the entire provided codebase to reverse-engineer a "reusable service blueprint." The goal is to generate a set of **Context Instructions** and **Skill Files** that will allow an AI or intermediate engineer to replicate this exact architecture for a new domain in minutes.

**Phase 1: Deep Ingestion (Mandatory)**
1.  Scan **every single file** in the project directory (Source code, POM/Gradle files, Dockerfiles, Kubernetes manifests, resource configs, and properties files).
2.  **Constraint:** Do not hallucinate or summarize yet. You must acknowledge that you have read the files recursively. Do not stop until the full dependency tree is mapped.

**Phase 2: Architectural Decomposition**
Analyze the code specifically looking for the "Core Entities" of this Event Handler pattern. Map the code logic to the following high-level design components:
1.  **Event Consumption:**
    * Identify how we connect to the broker (e.g., Azure/Kafka).
    * Identify the deserialization logic (Source Type: XML).
2.  **Logic Gating:**
    * Locate the "Is this the approved event type?" decision logic.
3.  **Data Transformation (The Core):**
    * Identify the mapping logic (Source -> Business View/BV).
    * Note libraries used (e.g., MapStruct, JSONata, custom Java mapping).
4.  **Enrichment:**
    * Identify where and how data is enriched (DB lookups, API calls).
5.  **Event Production:**
    * Identify the serialization logic (Target Type: Avro).
    * Identify the logic for publishing to the downstream topic.
6.  **Resilience & Error Handling:**
    * Locate the "Ignore," "Retry," and "DLQ/DB Backup" logic (specifically the "save to DB on publish failure" path).

**Phase 3: Asset Generation (The Output)**
Based on the analysis, generate the following **three** Markdown files. These will serve as the "Skills Folder" for future projects.

**File 1: `PROJECT_CONTEXT.md` (The "What")**
* Create a template that describes the system architecture.
* Include a section for "Core Dependencies" found in the code.
* Create a text-based representation of the flow: `Source(XML) -> Filter -> Map -> Enrich -> Sink(Avro)`.

**File 2: `CODING_STANDARDS.md` (The "How")**
* Extract the specific patterns used in this code.
* *Example:* "We use POJOs for internal representation," or "We use a specific Builder pattern for the Avro object."
* Define the error handling strategy strictly based on the existing code (e.g., "All publish failures must be caught and written to the `Failures` table, not just logged").

**File 3: `BUILD_PROMPT.xml` (The Generator)**
* Write a highly structured Prompt Template (using XML tags) that I can paste into a fresh AI session.
* This prompt must allow me to simply plug in:
    1.  The Source Schema (XML)
    2.  The Target Schema (Avro)
    3.  The Transformation Rules
* And the AI will output the full service code based on the architecture you just analyzed.

***

## 2. The Target "Skills Folder" Structure
*Organize the AI's output into this folder structure for your future repository.*

```text
/skills-event-handler-service/
├── architecture/
│   ├── design-pattern.png   (The HL Design diagram)
│   └── PROJECT_CONTEXT.md   (Generated by Phase 3)
├── guidelines/
│   └── CODING_STANDARDS.md  (Generated by Phase 3)
└── templates/
    ├── BUILD_PROMPT.xml     (The "Magic" prompt for the Agent)
    └── skeleton-structure/  (Optional: Empty folder structure of the project)
